{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is basically removing the suffix from a word and reduce it to its root word.Sometimes this root word have meaning sometimes don't have meaning.\n",
    "For example: “Flying” is a word and its suffix is “ing”, if we remove “ing” from “Flying” then we will get base word or root word which is “Fly”.\n",
    "    \n",
    "    \n",
    "Now Next question arise is why do we need it in Natural Language Processing or Natural Language Understanding:    \n",
    "The main aim is to reduce the inflectional forms of each word into a common base word or root word or stem word.    \n",
    "    \n",
    "    \n",
    "Inflection is a process of word formation, in which a word is modified to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, mood, animacy, and definiteness    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation:\n",
    "    There are majorly 2 errors in Stemming Algorithms which are as follows.\n",
    "    \n",
    "    1) Overstemming                                                                            \n",
    "    2) Understemming                                                                                             \n",
    "    \n",
    "#Overstemming \n",
    "Over-stemming is when two words with different stems are stemmed to the same root. This is also known as a false positive.\n",
    "\n",
    "Example:\n",
    "    \n",
    "    1) universal                                                                                 \n",
    "    2) university                                                                                        \n",
    "    3) universe                                                                                              \n",
    "\n",
    "All the above 3 words are stemmed to univers which is wrong behavior.\n",
    "\n",
    "Though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in NLP/NLU will likely reduce the relevance of the search results\n",
    "\n",
    "#Understemming                                                                                             \n",
    "Under-stemming is when two words that should be stemmed to the same root are not. This is also known as a false negative. Below is the example for the same.\n",
    "\n",
    "    1) alumnus                                                                                \n",
    "    2) alumni                                                                                   \n",
    "    3) alumnae                                                                                      \n",
    "\n",
    "    \n",
    "    \n",
    "As of now there are lot of ways by which we can stem a word and in this article we will be focusing on 3 stemming techniques which are part of truncating Stemming Algorithm, We wont be discussing about statistical or mixed Stemming Algorithm here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique of Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lot of technique of stemming but I will write only three type of stemming but not statistical or mixes type of stemming\n",
    "\n",
    "1) Porter stemmer                                                                              \n",
    "2) Snowball stemmer                                                                                   \n",
    "3) Lancaster stemmer                                                                                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porter Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most common and gentle stemmer, Its fast but not very precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provis',\n",
       " 'maximum',\n",
       " 'multipli',\n",
       " 'owe',\n",
       " 'care',\n",
       " 'on',\n",
       " 'go',\n",
       " 'gone',\n",
       " 'go',\n",
       " 'wa',\n",
       " 'thi']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porterstemming = PorterStemmer()\n",
    "\n",
    "sentence = \"Provision Maximum multiply owed caring on go gone going was this\"\n",
    "\n",
    "tokenized_word = nltk.word_tokenize(sentence)\n",
    "\n",
    "stemword = [porterstemming.stem(word) for word in tokenized_word]\n",
    "stemword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the input and you can see we are passing “was” and getting “wa” as output. This is something which should be considered under less precise algorithm. To increase the precision another algorithm came which was SnowBall Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were some improvements done on Porter Stemmer which made it more precise over large data-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provis',\n",
       " 'maximum',\n",
       " 'multipli',\n",
       " 'owe',\n",
       " 'care',\n",
       " 'on',\n",
       " 'go',\n",
       " 'gone',\n",
       " 'go',\n",
       " 'was',\n",
       " 'this']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstemming = SnowballStemmer(language='english')\n",
    "\n",
    "sentence = \"Provision Maximum multiply owed caring on go gone going was this\"\n",
    "\n",
    "tokenized_word = nltk.word_tokenize(sentence)\n",
    "\n",
    "stemword = [snowballstemming.stem(word) for word in tokenized_word]\n",
    "stemword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this was an improvement over Porter Stemmer hence we can see in the results how gracefully it handled “was” input. There was lots of improvement done in this algorithm. Hence currently it is one of my favorite algorithm to work with.\n",
    "\n",
    "There is one more impotant feature added to this algorithm which was excluding Stop Word Stemming.\n",
    "\n",
    "Due to this feature we observed difference in “was” input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provis',\n",
       " 'maximum',\n",
       " 'multipli',\n",
       " 'owe',\n",
       " 'care',\n",
       " 'on',\n",
       " 'go',\n",
       " 'gone',\n",
       " 'go',\n",
       " 'was',\n",
       " 'this']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstemming2 = SnowballStemmer('english', ignore_stopwords=True)\n",
    "\n",
    "sentence = \"Provision Maximum multiply owed caring on go gone going was this\"\n",
    "\n",
    "tokenized_word = nltk.word_tokenize(sentence)\n",
    "\n",
    "stemword = [snowballstemming2.stem(word) for word in tokenized_word]\n",
    "stemword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very aggressive algorithm.\n",
    "It will hugely trim down your working set, this statement itself has pros and cons, sometime you many want this in your datasets but maximum time you will be avoiding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provid',\n",
       " 'maxim',\n",
       " 'multiply',\n",
       " 'ow',\n",
       " 'car',\n",
       " 'on',\n",
       " 'go',\n",
       " 'gon',\n",
       " 'going',\n",
       " 'was',\n",
       " 'thi']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancasterstemmer = LancasterStemmer()\n",
    "\n",
    "sentence = \"Provision Maximum multiply owed caring on go gone going was this\"\n",
    "\n",
    "tokenized_word = nltk.word_tokenize(sentence)\n",
    "\n",
    "stemword = [lancasterstemmer.stem(word) for word in tokenized_word]\n",
    "stemword\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggression can be observed by “Caring” input, It was converted to “car” which is altogether a different word in English dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Conclusion\n",
    "\n",
    "Snowball Stemmer cames out to be one of the best suited algorithm for my needs, but this totally depends on use case and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
